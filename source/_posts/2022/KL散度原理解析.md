---
title: KL散度原理解析
top: false
cover: false
mathjax: false
date: 2022-06-19 14:17:23
summary: 
tags: 
  - 信息熵
  - 相对熵
  - 信息增益
categories: ML&DL
---

## KL散度

KL散度（Kullback-Leibler divergence，简称KLD），在信息系统中称为相对熵（relative entropy），在连续时间序列中称为随机性（randomness），在统计模型推断中称为信息增益（information gain），也称信息散度（information divergence）。KL散度用来度量使用基于 $q(x)$ 的分布来编码服从 $p(x)$ 的分布的样本所需的额外的平均比特数，其表示概率分布 $p(x)$ 和 $q(x)$ 差别的非对称性的度量，也就是说，$KL(p||q) \neq KL(q|p)$。典型情况下， $p(x)$ 表示数据的真实分布， $q(x)$ 表示数据的理论分布、估计的模型分布、或 $p(x)$ 的近似分布。

有人会将KL散度称为KL距离，但这并不严谨，因为KL散度并不满足距离的概念，因为KL不是对称的，也不满足三角不等式。KL散度来源信息熵，所以要理解KL散度，需要先理解信息熵。

## 信息熵

信息熵包含了两个概念，一个是信息，一个是熵，通俗而言，信息熵表示的就是信息的不确定性。那么为什么信息熵要表示成 $-\sum p*\log(p)$ 呢。

信息的不确定性程度也可理解为信息中的事件出现的概率，如果一个信息中表达的事件出现的概率很大，那么即使没有接受到这个信息，我们也能大概知道这个事件会发生，这个信息本身含有的信息量就会很低。比如一个信息说，“明天太阳会从东方升起”，信息量如此之少，以至于我们没有必要接受这条信息，而如果一个消息说，“明天早上会有月食”，这个信息量就很丰富。

其实信息论的基本思想就是这样来的，一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。

有了以上的铺垫，现在我们来量化一下信息：

1. 事件发生的概率越高，信息量越小，特殊情况下，必然事件没有信息量
2. 事件发生的概率越低，信息量越大
3. 多个独立事件的总信息是每个独立事件的信息和

从1、2可以知道，信息量是事件发生概率的减函数，而从3可以知道，信息量与事件概率之间应该是对数关系（因为独立事件的总概率是独立事件概率的乘积，而信息则是和，严格的数学定义为：已知 $f(x)$ 的定义域为 $x \in (0, 1]$，可导，满足 $f(x, y) = f(x) + f(y)$ ，求 $f(x)$。

用两个事件简单表示如下：

- $x_{1}$ 和 $x_{2}$ 同时发生的概率是：$P(x_{1}, x_{2}) = P(x_{1})P(x_{2})$ 
- $x_{1}$ 和 $x_{2}$ 同时发生的总信息量是：$\log(P(x_{1})P(x_{2})) = \log(P(x_{1})) + \log(P(x_{2}))$ 

所以为了满足上面的三个性质，于是可以定义一个事件的自信息为

$I(x) = -\log P(x)$ 

上面表示使用的是自然对数，这时候 $I(x)$ 的单位是奈特(nats)，一奈特就是以 $\frac{1}{e}$ 的概率观测到一个事件时获得的信息量。当底数变成2时，单位就是我们熟知比特(bit)，也叫香农。不管底数变成什么，信息量都只是奈特信息量的常数倍。

知道了如何度量单个信息的不确定性，那对于一个离散的或连续的一系列信息 $x$ ，如何来度量信息的的不确定性。

既然是一系列的信息，那就肯定服从某一种概率分布，那么整个概率分布中的不确定性总量就可以度量为分布中每个信息的期望，于是就量化为了

$$H(x) = E_{x \backsim P}(I(x)) = - E_{x \backsim P}(log P(x))$$ 

如此，就已经把信息的不确定性量化了。

当x是离散的时候，就是我们熟知的香农熵

$$H(x) = - \sum^{n}_{i} x_{i} * \log(x_{i})$$

当x是连续的时候，香农熵也被称为微分熵

$$H(x) = - \int^{\infty}_{-\infty} x * \log(x) dx$$

## 从信息熵到KL散度

KL散度的目的是假定一个近似的分布 $q(x)$ 来对未知的分布 $p(x)$ 进行建模。如果我们用 $q(x)$ 来建立一个编码体系，那相比于使用真实分布 $p(x)$ 进行编码增加的信息量为：

$D_{KL}(p||q) = - \sum^{n}_{i} p(x_{i}) \log(q(x_{i})) - (- \sum^{n}_{i} p(x_{i}) \log(p(x_{i}))) = - \sum^{n}_{i} p(x_{i}) \log (\frac{q(x_{i})}{p(x_{i})})$ 

如果是连续值，就变成积分

$$D_{KL}(p||q) = - \int^{\infty}_{-\infty} p(x) \log(q(x)) dx - (- \int^{\infty}_{-\infty} p(x) \log(p(x))) dx = - \int^{\infty}_{-\infty} p(x) \log (\frac{q(x)}{p(x)}) dx$$ 

也就是说，当我们知道真实的概率分布之后，可以给出最有效的编码。如果我们使用了不同于真实分布的概率分布，那么我们一定会损失编码效率，并且在传输时增加的平均额外信息量至少等于两个分布之间的KL散度。

$D_{KL}(p||q) \geq 0$ ，且由 Jensen 不等式知道，当且仅当 $p = q$ 时，$D_{KL}(p||q) = 0$。因此我们经常把KL散度看做两个分布 $p(x)$ 和 $q(x)$ 之间不相似程度的度量。

## 从KL散度到JS散度再到Wasserstein距离

JS散度是KL散度的变体，解决了KL散度非对称的问题，同样度量两个概率分布的相似度，。

$$D_{JS}(p||q) = \frac{1}{2}D_{KL}(p||\frac{p + q}{2}) + \frac{1}{2}D_{KL}(q||\frac{p + q}{2})$$

但是KL散度和JS散度度量的问题仍然存在问题。

如果两个分配P,Q离得很远，完全没有重叠的时候，那么KL散度值是没有意义的，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为0。梯度消失了。

$$W(p, q) = \inf_{\gamma \backsim \prod(p, q)} E_{(x, y) \backsim \gamma}[||x - y||]$$

$\prod(p, q)$ 是 $p, q$ 分布组合起来的所有可能的联合分布的集合，对每一个可能的联合分布 $\gamma$ ，可以从中采样 $(x, y) \backsim \gamma$ 得到一个样本x和y，并计算出这对样本的距离||x−y||，所以可以计算该联合分布 $\gamma$ 下，样本对距离的期望值 $E_{(x, y) \backsim \gamma}[||x - y||]$ 。在所有可能的联合分布中能够对这个期望值取到的下界inf，就是Wasserstein距离。

直观上可以把 $E_{(x, y) \backsim \gamma}[||x - y||]$ 理解为在 $\gamma$ 这个路径规划下把土堆P1挪到土堆P2所需要的消耗。而Wasserstein距离就是在最优路径规划下的最小消耗。所以Wesserstein距离又叫Earth-Mover距离。

Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近；而JS散度在此情况下是常量，KL散度可能无意义。[KL散度、JS散度、Wasserstein距离](https://zxth93.github.io/2017/09/27/KL%E6%95%A3%E5%BA%A6JS%E6%95%A3%E5%BA%A6Wasserstein%E8%B7%9D%E7%A6%BB/index.html) 



